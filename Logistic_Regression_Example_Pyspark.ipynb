{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd50ada",
   "metadata": {},
   "source": [
    "### 일반 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c50bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8273bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/13 11:58:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Logit').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255d4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c31b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/13 11:58:15 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('libsvm').load('sample_libsvm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ff788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4bce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/13 11:58:25 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/06/13 11:58:25 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/06/13 11:58:25 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/06/13 11:58:25 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/06/13 11:58:27 WARN BlockManager: Asked to remove block broadcast_25_piece0, which does not exist\n"
     ]
    }
   ],
   "source": [
    "logit_model = logit.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a7c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = logit_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45b44e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[20.3777627514872...|[0.99999999858729...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-21.114014198868...|[6.76550380000472...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-23.743613234676...|[4.87842678716177...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-19.192574012720...|[4.62137287298144...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.125398874699...|[1.81823629113068...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[20.4890549504196...|[0.99999999873608...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-21.082940212814...|[6.97903542823766...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.622713503550...|[3.00582577446132...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.1594863606582...|[0.99999999935352...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[28.1036706837287...|[0.99999999999937...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-21.054076780106...|[7.18340962960324...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.9648490510184...|[0.99999999999805...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[32.7855654161400...|[0.99999999999999...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-20.331839179667...|[1.47908944089721...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.7830579106564...|[0.99999999965347...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-20.640562103728...|[1.08621994880353...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[22.6400775503731...|[0.99999999985292...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[38.0712919910909...|           [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-19.830803265627...|[2.44113371545874...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-21.016054806036...|[7.46179590484091...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lg.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274367fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce2ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logit.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2fbe894",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b415555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9743589743589743"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb6f59fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9411764705882353, 1.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.precisionByLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7e2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149eb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_eval = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4304f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred = r.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "779b475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bi_roc = bi_eval.evaluate(r.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e65bc7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972826086956521"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca402d4",
   "metadata": {},
   "source": [
    "### 타이타닉 예제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48b7ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df = spark.read.csv('titanic.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08ead7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 확인\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4429491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컬럼 확인\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3e453fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.select(['Survived',\n",
    " 'Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7796f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived 0\n",
      "Pclass 0\n",
      "Sex 0\n",
      "Age 177\n",
      "SibSp 0\n",
      "Parch 0\n",
      "Fare 0\n",
      "Embarked 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "# null값 확인\n",
    "for i, j in zip(cols.columns, cols.select([count(when(col(c).isNull(), c)) for c in cols.columns]).collect()[0]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad99f8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 개수 파악\n",
    "cols.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31cbdf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived 0.010539215871285685\n",
      "Pclass -0.36135321538780957\n",
      "Age 1.0\n",
      "SibSp -0.18466352835224448\n",
      "Parch -0.04878608272014969\n",
      "Fare 0.135515853527051\n"
     ]
    }
   ],
   "source": [
    "# Age를 파악할 수 있을지 Feature 별 상관계수 파악\n",
    "for i in cols.columns:\n",
    "    if str(cols.schema[i].dataType) != 'StringType' :\n",
    "        print(i, cols.corr('Age', i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78457164",
   "metadata": {},
   "source": [
    "상관계수가 특출나게 높은 feature가 없으므로, mean 또는 삭제해서 진행해야함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b94da2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 삭제처리\n",
    "cols = cols.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d89fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87d66f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성별을 StringIndexer로 OrdinalEncoding하기\n",
    "gender_idxer = StringIndexer(inputCol = 'Sex', outputCol = 'SexIndex')\n",
    "\n",
    "# 성별 number를 OneHotEncoding 하기\n",
    "gender_encoder = OneHotEncoder(inputCol = 'SexIndex',outputCol = 'SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4295cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embark를 StringIndexer로 OrdinalEncoding하기\n",
    "embark_idxer = StringIndexer(inputCol = 'Embarked', outputCol = 'EmbarkIndex')\n",
    "\n",
    "# embarkIndex를 OneHotEncoding 하기\n",
    "embark_encoder = OneHotEncoder(inputCol = 'EmbarkIndex',outputCol = 'EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e0efd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 벡터화 어셈블러 정의\n",
    "assembler = VectorAssembler(inputCols=['Pclass','SexVec','EmbarkVec','Age','SibSp','Parch','Fare'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d851fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eedc33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복잡한 구조를 한번에 만들어 줄 파이프라인 활용\n",
    "\n",
    "logit = LogisticRegression(featuresCol='features', labelCol= 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9f8ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [gender_idxer, # 성별 수치화\n",
    "                              embark_idxer, # embark 수치화\n",
    "                              gender_encoder, # 성별 벡터화(원핫)\n",
    "                              embark_encoder, # embark 벡터화(원핫)\n",
    "                              assembler, # 전체 feature 벡터화\n",
    "                              logit]) # 로지스틱 회귀모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "282208d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = cols.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0ad7f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "808a279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = logit_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bbed07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9baffdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 진행하기 위한 prediction columne을 지정한 이유는 항상 예측(transform)을 진행하면 prediction col이 생기기 때문\n",
    "eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol= 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8317886f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측값과 실제값 비교해보기\n",
    "results.select('Survived','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f673f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  288|\n",
      "|       0|  424|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label 분포 파악해보기\n",
    "cols.groupBy('Survived').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "41227a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d73d7060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171355498721228"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97e718",
   "metadata": {},
   "source": [
    "이상 이항분류평가를 이용한 작업이며, 다항분류평가를 이용해 다양한(정밀도, 재현율, 정확도) 메트릭을 파악할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdaf1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
